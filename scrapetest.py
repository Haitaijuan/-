# coding=utf-8	# from urllib.request import urlopen# from urllib.error import HTTPError# from bs4 import BeautifulSoup# def getTitle(url):# 	try:# 		html = urlopen(url)# 	except HTTPError as e :# 		return None# 	try:# 		bsObj = BeautifulSoup(html.read())# 		title = bsObj.body.h1	# 	except AttributeError as e:# 		return None# 	return title# title = getTitle("http://www.pythonscraping.com/exercises/exercise1.html")# if title == None:# 	print("Title could not be found")# else:# 	print(title)# from urllib.request import urlopen# from bs4 import BeautifulSoup# html = urlopen("http://www.pythonscraping.com/pages/warandpeace.html")# bsObj = BeautifulSoup(html,"html.parser")# nameList = bsObj.findAll("span", {"class":"green"})# # for name in nameList:# # 	print(name.get_text()) # allText = bsObj.findAll(id="text")# print(allText[0].get_text())# from urllib.request import urlopen# from bs4 import BeautifulSoup# html = urlopen("http://www.pythonscraping.com/pages/page3.html")# bsObj = BeautifulSoup(html,"html.parser")# # for child in bsObj.find("table",{"id":"giftList"}).children:# # 	print(child)# for sibling in bsObj.find("table",{"id":"giftList"}).tr.next_siblings:# 	print(sibling)# from urllib.request import urlopen# from bs4 import BeautifulSoup# html = urlopen("http://www.pythonscraping.com/pages/page3.html")# bsObj = BeautifulSoup(html,"html.parser")# print(bsObj.find("img",{"src":"../img/gifts/img1.jpg"# 						}).parent.previous_sibling.get_text())# from urllib.request import urlopen# from bs4 import BeautifulSoup# import re# html = urlopen("http://www.pythonscraping.com/pages/page3.html")# bsObj = BeautifulSoup(html,"html.parser")# images = bsObj.findAll("img",{"src":re.compile("\.\.\/img\/gifts/img.*\.jpg")})# for image in images:# 	print(image["src"])# # python2 version# import urllib# from bs4 import BeautifulSoup# import ssl# html = urllib.request.urlopen("https://en.wikipedia.org/wiki/Kevin_Bacon") # bsObj = BeautifulSoup(html,"html.parser")# for link in bsObj.findAll("a"):# 	if 'href' in link.attrs: # 		print(link.attrs['href'])# from urllib.request import urlopen # from bs4 import BeautifulSoup# html = urlopen("http://en.wikipedia.org/wiki/Kevin_Bacon") # bsObj = BeautifulSoup(html,"html.parser")# for link in bsObj.findAll("a"):# 	if 'href' in link.attrs: # 		print(link.attrs['href'])# from urllib.request import urlopen # from bs4 import BeautifulSoup # import re# html = urlopen("http://en.wikipedia.org/wiki/Kevin_Bacon") # bsObj = BeautifulSoup(html,"html.parser")# for link in bsObj.find("div", {"id":"bodyContent"}).findAll("a",# 					   href=re.compile("^(/wiki/)((?!:).)*$")): # 	if 'href' in link.attrs:# 		print(link.attrs['href'])# from urllib.request import urlopen# from bs4 import BeautifulSoup# import re# import datetime# import random# random.seed(datetime.datetime.now())# def getLinks(articleUrl):# 	html = urlopen("http://en.wikipedia.org"+articleUrl)# 	bsObj = BeautifulSoup(html,"html.parser")# 	return bsObj.find("div", {"id":"bodyContent"}).findAll("a",# 								href=re.compile("^(/wiki/)((?!:).)*$"))# links = getLinks("/wiki/Kevin_Bacon")# while len(links) > 0:	# 	newArticle = links[random.randint(0, len(links)-1)].attrs["href"]# 	print(newArticle)# 	links = getLinks(newArticle)# from urllib.request import urlopen # from bs4 import BeautifulSoup # import datetime# import random# import re# random.seed(datetime.datetime.now()) # def getLinks(articleUrl):# 	html = urlopen("http://en.wikipedia.org"+articleUrl)# 	bsObj = BeautifulSoup(html,"html.parser")# 	return bsObj.find("div", {"id":"bodyContent"}).findAll("a",# 	                         href=re.compile("^(/wiki/)((?!:).)*$"))# links = getLinks("/wiki/Kevin_Bacon")# while len(links) > 0:# 	newArticle = links[random.randint(0, len(links)-1)].attrs["href"] # 	print(newArticle)# 	links = getLinks(newArticle)# from urllib.request import urlopen # from bs4 import BeautifulSoup # import re# pages = set()# def getLinks(pageUrl):# 	global pages# 	html = urlopen("http://en.wikipedia.org"+pageUrl)# 	bsObj = BeautifulSoup(html,"html.parser")# 	for link in bsObj.findAll("a", href=re.compile("^(/wiki/)")):# 		if 'href' in link.attrs:# 			if link.attrs['href'] not in pages:# 				#We have encountered a new page# 				newPage = link.attrs['href'] # 				print(newPage) # 				pages.add(newPage) # 				getLinks(newPage)# getLinks("")# from urllib.request import urlopen # from bs4 import BeautifulSoup # import re# pages = set()# def getLinks(pageUrl):# 	global pages# 	html = urlopen("http://en.wikipedia.org"+pageUrl) # 	bsObj = BeautifulSoup(html, "html.parser")# 	try:# 		print(bsObj.h1.get_text())# 		print(bsObj.find(id ="mw-content-text").findAll("p")[0]) # 		print(bsObj.find(id="ca-edit").find("span").find("a").attrs['href'])# 	except AttributeError:# 		print("This page is missing something! No worries though!")# 	for link in bsObj.findAll("a", href=re.compile("^(/wiki/)")):# 		if 'href' in link.attrs:# 			if link.attrs['href'] not in pages: # 				#We have encountered a new page # 				newPage = link.attrs['href'] # 				print("----------------\n"+newPage) # 				pages.add(newPage) # 				getLinks(newPage)# getLinks("")from urllib.request import urlopen from bs4 import BeautifulSoup import reimport datetimeimport random pages = set()random.seed(datetime.datetime.now())#Retrieves a list of all Internal links found on a pagedef getInternalLinks(bsObj, includeUrl):	internalLinks = []	#Finds all links that begin with a "/"	for link in bsObj.findAll("a", href=re.compile("^(/|.*"+includeUrl+")")):		if link.attrs['href'] is not None:			if link.attrs['href'] not in internalLinks:				internalLinks.append(link.attrs['href']) 	return internalLinks#Retrieves a list of all external links found on a pagedef getExternalLinks(bsObj, excludeUrl):	externalLinks = []	#Finds all links that start with "http" or "www" that do 	#not contain the current URL	for link in bsObj.findAll("a",							  href=re.compile("^(http|www)((?!"+excludeUrl+").)*$")): 		if link.attrs['href'] is not None:			if link.attrs['href'] not in externalLinks: 				externalLinks.append(link.attrs['href'])	return externalLinksdef splitAddress(address):	#提取原地址内以/分割的不同段	#去掉http://部分	addressParts = address.replace("http://", "").split("/") 	return addressPartsdef getRandomExternalLink(startingPage):	html = urlopen(startingPage)	bsObj = BeautifulSoup(html,"html.parser")	externalLinks = getExternalLinks(bsObj, splitAddress(startingPage)[0]) 	if len(externalLinks) == 0:		#进入下一层 进行递归继续寻找		internalLinks = getInternalLinks(startingPage)		return getNextExternalLink(internalLinks[random.randint(0,									len(internalLinks)-1)])	else:		return externalLinks[random.randint(0, len(externalLinks)-1)]def followExternalOnly(startingSite):	externalLink = getRandomExternalLink(startingSite) 	print("Random external link is: "+externalLink) 	followExternalOnly(externalLink)followExternalOnly("http://oreilly.com")